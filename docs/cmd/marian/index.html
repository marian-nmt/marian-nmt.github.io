<!--[if IE 8]> <html lang="en" class="ie8"> <![endif]-->
<!--[if IE 9]> <html lang="en" class="ie9"> <![endif]-->
<!--[if !IE]><!-->
<html lang="en">
<!--<![endif]-->

  <head>
  <title>
    
    Marian :: Command-line options for marian
    
  </title>
  <!-- Meta -->
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Fast Neural Machine Translation in C++">

  <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
  <!-- Global CSS -->
  <link rel="stylesheet" href="/assets/plugins/bootstrap/css/bootstrap.min.css">
  <!-- Plugins CSS -->
  <link rel="stylesheet" href="/assets/plugins/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="/assets/css/pygments/github.css">

  <!-- Theme CSS -->
  <link id="theme-style" rel="stylesheet" href="/assets/css/styles.css">
  <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->

  <link rel="stylesheet" href="/assets/plugins/github-fork-ribbon-css/gh-fork-ribbon.css" />
  <!--[if lt IE 9]>
    <link rel="stylesheet" href="/assets/plugins/github-fork-ribbon-css/gh-fork-ribbon.ie.css" />
  <![endif]-->

  

  
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109819276-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-109819276-1');
</script>

  

</head>


  <body class="body-blue">
    <a class="github-fork-ribbon" href="https://github.com/marian-nmt/marian" title="Fork me on GitHub">Fork me on GitHub</a>

    <div class="page-wrapper">

    <header id="header" class="header">
  <div class="container">
    <div class="branding">
      <h1 class="logo">
        <a href="/">
          <span aria-hidden="true" class="icon_documents_alt icon"></span>
          <span class="text-highlight">Marian</span><span class="text-bold">NMT</span>
        </a>
      </h1>
      <p class="description">Fast Neural Machine Translation in C++</p>
    </div><!--//branding-->

    <ol class="breadcrumb">


 

 

 

 

 

 

 

 

 

 

 

 

 

 

 
 <li>
   <a class="page-link" href="/quickstart/">Quick start</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/features/">Features &amp; Benchmarks</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/docs/">Documentation</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/examples/">Examples</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/faq">FAQ</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/publications/">Publications</a>
 </li>
 

</ol>


  </div><!--//container-->
</header><!--//header-->


    <div class="doc-wrapper">
      <div class="container">

        <div id="doc-header" class="doc-header text-center">
          <h1 class="doc-title">
            
            <i class="icon fa fa-file-code-o }}"></i>
            
            Command-line options for marian
          </h1>
          <div class="meta">
            <i class="fa fa-clock-o"></i>
            Last updated: 26 June 2018
          </div>
        </div><!--//doc-header-->

        <div class="doc-body">
          <div class="doc-content">
            <div class="content-inner">

              <h2 id="general-options">General options</h2>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-c [ --config ] arg                                  Configuration file(s). If multiple, later overrides earlier.
-w [ --workspace ] arg (=2048)                       Preallocate  arg  MB of work space
--log arg                                            Log training process information to file given by  arg
--log-level arg (=info)                              Set verbosity level of logging (trace - debug - info - warn - err(or) - critical - off)
--quiet                                              Suppress all logging to stderr. Logging to files still works
--quiet-translation                                  Suppress logging for translation
--seed arg (=0)                                      Seed for all random number generators. 0 means initialize randomly
--clip-gemm arg (=0)                                 If not 0 clip GEMM input values to +/- arg
--interpolate-env-vars                               allow the use of environment variables in paths, of the form ${VAR_NAME}
--relative-paths                                     All paths are relative to the config file location
--dump-config                                        Dump current (modified) configuration to stdout and exit
--version                                            Print version number and exit
-h [ --help ]                                        Print this help message and exit
</code></pre></div></div>

<h2 id="model-options">Model options</h2>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-m [ --model ] arg (=model.npz)                      Path prefix for model to be saved/resumed
--pretrained-model arg                               Path prefix for pre-trained model to initialize model weights
--ignore-model-config                                Ignore the model configuration saved in npz file
--type arg (=amun)                                   Model type (possible values: amun, nematus, s2s, multi-s2s, transformer)
--dim-vocabs arg (=0 0)                              Maximum items in vocabulary ordered by rank, 0 uses all items in the provided/created vocabulary file
--dim-emb arg (=512)                                 Size of embedding vector
--dim-rnn arg (=1024)                                Size of rnn hidden state
--enc-type arg (=bidirectional)                      Type of encoder RNN : bidirectional, bi-unidirectional, alternating (s2s)
--enc-cell arg (=gru)                                Type of RNN cell: gru, lstm, tanh (s2s)
--enc-cell-depth arg (=1)                            Number of transitional cells in encoder layers (s2s)
--enc-depth arg (=1)                                 Number of encoder layers (s2s)
--dec-cell arg (=gru)                                Type of RNN cell: gru, lstm, tanh (s2s)
--dec-cell-base-depth arg (=2)                       Number of transitional cells in first decoder layer (s2s)
--dec-cell-high-depth arg (=1)                       Number of transitional cells in next decoder layers (s2s)
--dec-depth arg (=1)                                 Number of decoder layers (s2s)
--skip                                               Use skip connections (s2s)
--layer-normalization                                Enable layer normalization
--right-left                                         Train right-to-left model
--best-deep                                          Use Edinburgh deep RNN configuration (s2s)
--special-vocab arg                                  Model-specific special vocabulary ids
--tied-embeddings                                    Tie target embeddings and output embeddings in output layer
--tied-embeddings-src                                Tie source and target embeddings
--tied-embeddings-all                                Tie all embedding layers and output layer
--transformer-heads arg (=8)                         Number of heads in multi-head attention (transformer)
--transformer-no-projection                          Omit linear projection after multi-head attention (transformer)
--transformer-dim-ffn arg (=2048)                    Size of position-wise feed-forward network (transformer)
--transformer-ffn-depth arg (=2)                     Depth of filters (transformer)
--transformer-ffn-activation arg (=swish)            Activation between filters: swish or relu (transformer)
--transformer-dim-aan arg (=2048)                    Size of position-wise feed-forward network in AAN (transformer)
--transformer-aan-depth arg (=2)                     Depth of filter for AAN (transformer)
--transformer-aan-activation arg (=swish)            Activation between filters in AAN: swish or relu (transformer)
--transformer-aan-nogate                             Omit gate in AAN (transformer)
--transformer-decoder-autoreg arg (=self-attention)  Type of autoregressive layer in transformer decoder: self-attention, average-attention (transformer)
--transformer-preprocess arg                         Operation before each transformer layer: d = dropout, a = add, n = normalize
--transformer-postprocess-emb arg (=d)               Operation after transformer embedding layer: d = dropout, a = add, n = normalize
--transformer-postprocess arg (=dan)                 Operation after each transformer layer: d = dropout, a = add, n = normalize
--dropout-rnn arg (=0)                               Scaling dropout along rnn layers and time (0 = no dropout)
--dropout-src arg (=0)                               Dropout source words (0 = no dropout)
--dropout-trg arg (=0)                               Dropout target words (0 = no dropout)
--grad-dropping-rate arg (=0)                        Gradient Dropping rate (0 = no gradient Dropping)
--grad-dropping-momentum arg (=0)                    Gradient Dropping momentum decay rate (0.0 to 1.0)
--grad-dropping-warmup arg (=100)                    Do not apply gradient dropping for the first arg steps
--transformer-dropout arg (=0)                       Dropout between transformer layers (0 = no dropout)
--transformer-dropout-attention arg (=0)             Dropout for transformer attention (0 = no dropout)
--transformer-dropout-ffn arg (=0)                   Dropout for transformer filter (0 = no dropout)
</code></pre></div></div>

<h2 id="training-options">Training options</h2>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>--cost-type arg (=ce-mean)                           Optimization criterion: ce-mean, ce-mean-words, ce-sum, perplexity
--overwrite                                          Overwrite model with following checkpoints
--no-reload                                          Do not load existing model specified in --model arg
-t [ --train-sets ] arg                              Paths to training corpora: source target
-v [ --vocabs ] arg                                  Paths to vocabulary files have to correspond to --train-sets. If this parameter is not supplied we look for vocabulary files
                                                     source.{yml,json} and target.{yml,json}. If these files do not exist they are created
--max-length arg (=50)                               Maximum length of a sentence in a training sentence pair
--max-length-crop                                    Crop a sentence to max-length instead of ommitting it if longer than max-length
-e [ --after-epochs ] arg (=0)                       Finish after this many epochs, 0 is infinity
--after-batches arg (=0)                             Finish after this many batch updates, 0 is infinity
--disp-freq arg (=1000)                              Display information every  arg  updates
--disp-label-counts                                  Display label counts when logging loss progress
--save-freq arg (=10000)                             Save model file every  arg  updates
--no-shuffle                                         Skip shuffling of training data before each epoch
--no-restore-corpus                                  Skip restoring corpus state after training is restarted
-T [ --tempdir ] arg (=/tmp)                         Directory for temporary (shuffled) files and database
--sqlite [=arg(=temporary)]                          Use disk-based sqlite3 database for training corpus storage, default is temporary with path creates persistent storage
--sqlite-drop                                        Drop existing tables in sqlite3 database
-d [ --devices ] arg (=0)                            GPU ID(s) to use for training
--cpu-threads [=arg(=1)] (=0)                        Use CPU-based computation with this many independent threads, 0 means GPU-based computation
--mini-batch arg (=64)                               Size of mini-batch used during update
--mini-batch-words arg (=0)                          Set mini-batch size based on words instead of sentences
--mini-batch-fit                                     Determine mini-batch size automatically based on sentence-length to fit reserved memory
--mini-batch-fit-step arg (=10)                      Step size for mini-batch-fit statistics
--maxi-batch arg (=100)                              Number of batches to preload for length-based sorting
--maxi-batch-sort arg (=trg)                         Sorting strategy for maxi-batch: trg (default) src none
-o [ --optimizer ] arg (=adam)                       Optimization algorithm (possible values: sgd, adagrad, adam
--optimizer-params arg                               Parameters for optimization algorithm, e.g. betas for adam
--optimizer-delay arg (=1)                           SGD update delay, 1 = no delay
-l [ --learn-rate ] arg (=0.0001)                    Learning rate
--lr-decay arg (=0)                                  Decay factor for learning rate: lr = lr * arg (0 to disable)
--lr-decay-strategy arg (=epoch+stalled)             Strategy for learning rate decaying (possible values: epoch, batches, stalled, epoch+batches, epoch+stalled)
--lr-decay-start arg (=10 1)                         The first number of epoch/batches/stalled validations to start learning rate decaying
--lr-decay-freq arg (=50000)                         Learning rate decaying frequency for batches, requires --lr-decay-strategy to be batches
--lr-decay-reset-optimizer                           Reset running statistics of optimizer whenever learning rate decays
--lr-decay-repeat-warmup                             Repeat learning rate warmup when learning rate is decayed
--lr-decay-inv-sqrt arg (=0)                         Decrease learning rate at arg / sqrt(no. updates) starting at arg
--lr-warmup arg (=0)                                 Increase learning rate linearly for arg first steps
--lr-warmup-start-rate arg (=0)                      Start value for learning rate warmup
--lr-warmup-cycle                                    Apply cyclic warmup
--lr-warmup-at-reload                                Repeat warmup after interrupted training
--lr-report                                          Report learning rate for each update
--batch-flexible-lr                                  Scales the learning rate based on the number of words in a mini-batch
--batch-normal-words arg (=1920)                     Set number of words per batch that the learning rate corresponds to. The option is only active when batch-flexible-lr is on
--sync-sgd                                           Use synchronous SGD instead of asynchronous for multi-gpu training
--label-smoothing arg (=0)                           Epsilon for label smoothing (0 to disable)
--clip-norm arg (=1)                                 Clip gradient norm to  arg  (0 to disable)
--exponential-smoothing [=arg(=1e-4)] (=0)           Maintain smoothed version of parameters for validation and saving with smoothing factor arg.  0 to disable.
--guided-alignment arg                               Use guided alignment to guide attention
--guided-alignment-cost arg (=ce)                    Cost type for guided alignment. Possible values: ce (cross-entropy), mse (mean square error), mult (multiplication)
--guided-alignment-weight arg (=1)                   Weight for guided alignment cost
--data-weighting arg                                 File with sentence or word weights
--data-weighting-type arg (=sentence)                Processing level for data weighting. Possible values: sentence, word
--embedding-vectors arg                              Paths to files with custom source and target embedding vectors
--embedding-normalization                            Enable normalization of custom embedding vectors
--embedding-fix-src                                  Fix source embeddings. Affects all encoders
--embedding-fix-trg                                  Fix target embeddings. Affects all decoders
--multi-node                                         Enable multi-node training through MPI
--multi-node-overlap arg (=1)                        Overlap model computations with MPI communication
</code></pre></div></div>

<h2 id="validation-set-options">Validation set options</h2>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>--valid-sets arg                                     Paths to validation corpora: source target
--valid-freq arg (=10000)                            Validate model every  arg  updates
--valid-metrics arg (=cross-entropy)                 Metric to use during validation: cross-entropy, perplexity, valid-script, translation. Multiple metrics can be specified
--valid-mini-batch arg (=32)                         Size of mini-batch used during validation
--valid-max-length arg (=1000)                       Maximum length of a sentence in a validating sentence pair
--valid-script-path arg                              Path to external validation script. It should print a single score to stdout. If the option is used with validating 
                                                     translation, the output translation file will be passed as a first argument 
--early-stopping arg (=10)                           Stop if the first validation metric does not improve for  arg  consecutive validation steps
--keep-best                                          Keep best model for each validation metric
--valid-log arg                                      Log validation scores to file given by  arg
--valid-translation-output arg                       Path to store the translation
-b [ --beam-size ] arg (=12)                         Beam size used during search with validating translator
-n [ --normalize ] [=arg(=1)] (=0)                   Divide translation score by pow(translation length, arg) 
--word-penalty [=arg(=0)] (=0)                       Subtract (arg * translation length) from translation score 
--max-length-factor arg (=3)                         Maximum target length as source length times factor
--allow-unk                                          Allow unknown words to appear in output
--n-best                                             Generate n-best list
</code></pre></div></div>
<p>Version: 
v1.5.0+b0283484</p>


            </div><!--//content-inner-->
          </div><!--//doc-content-->

          <div class="doc-sidebar hidden-xs">
            <nav id="doc-nav"></nav>
          </div><!--//doc-sidebar-->

        </div><!--//doc-body-->

      </div><!--//container-->
    </div><!--//doc-wrapper-->

    </div><!--//page-wrapper-->

    <footer id="footer" class="footer text-center">
  <div class="container">
    <p>
     Marian - an efficient Neural Machine Translation framework written in pure C++.</br>
      Mainly developed at the Adam Mickiewicz University in Pozna≈Ñ and at the University of Edinburgh.
    </p>
    <p><a href="https://github.com/marian-nmt/marian">Marian</a> is licensed under the <a href="https://github.com/marian-nmt/marian/LICENSE">MIT license</a>.</p>
    <small class="copyright">Based on the theme PrettyDocs designed by <a href="http://themes.3rdwavemedia.com/" targe="_blank">Xiaoying Riley</a> with modifications.</small>
  </div><!--//container-->
</footer><!--//footer-->

    <!-- Main Javascript -->
<script type="text/javascript"> localStorage.clear();</script>

<script type="text/javascript" src="/assets/plugins/jquery-1.12.3.min.js"></script>
<script type="text/javascript" src="/assets/plugins/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/assets/plugins/jquery-scrollTo/jquery.scrollTo.min.js"></script>
<script type="text/javascript" src="/assets/plugins/jquery-match-height/jquery.matchHeight-min.js"></script>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/javascript" src="/assets/js/main.js"></script>
<script type="text/javascript" src="/assets/js/toc.js"></script>


  </body>
</html>
