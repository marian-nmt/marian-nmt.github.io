<!--[if IE 8]> <html lang="en" class="ie8"> <![endif]-->
<!--[if IE 9]> <html lang="en" class="ie9"> <![endif]-->
<!--[if !IE]><!-->
<html lang="en">
<!--<![endif]-->

  <head>
  <title>
    
    Marian :: Documentation
    
  </title>
  <!-- Meta -->
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Fast Neural Machine Translation in C++">

  <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
  <!-- Global CSS -->
  <link rel="stylesheet" href="/assets/plugins/bootstrap/css/bootstrap.min.css">
  <!-- Plugins CSS -->
  <link rel="stylesheet" href="/assets/plugins/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="/assets/css/pygments/github.css">

  <!-- Theme CSS -->
  <link id="theme-style" rel="stylesheet" href="/assets/css/styles.css">
  <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->

  <link rel="stylesheet" href="/assets/plugins/github-fork-ribbon-css/gh-fork-ribbon.css" />
  <!--[if lt IE 9]>
    <link rel="stylesheet" href="/assets/plugins/github-fork-ribbon-css/gh-fork-ribbon.ie.css" />
  <![endif]-->

  
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109819276-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-109819276-1');
</script>

  
</head>


  <body class="body-blue">
    <a class="github-fork-ribbon" href="https://github.com/marian-nmt/marian" title="Fork me on GitHub">Fork me on GitHub</a>

    <div class="page-wrapper">

    <header id="header" class="header">
  <div class="container">
    <div class="branding">
      <h1 class="logo">
        <a href="/">
          <span aria-hidden="true" class="icon_documents_alt icon"></span>
          <span class="text-highlight">Marian</span><span class="text-bold">NMT</span>
        </a>
      </h1>
      <p class="description">Fast Neural Machine Translation in C++</p>
    </div><!--//branding-->

    <ol class="breadcrumb">


 

 

 

 

 

 

 

 

 

 

 

 

 

 
 <li>
   <a class="page-link" href="/quickstart/">Quick start</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/features/">Features &amp; Benchmarks</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/docs/">Documentation</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/examples/">Examples</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/faq">FAQ</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/publications/">Publications</a>
 </li>
 

</ol>


  </div><!--//container-->
</header><!--//header-->


    <div class="doc-wrapper">
      <div class="container">

        <div id="doc-header" class="doc-header text-center">
          <h1 class="doc-title">
            
            <i class="icon fa fa-file-code-o }}"></i>
            
            Documentation
          </h1>
          <div class="meta">
            <i class="fa fa-clock-o"></i>
            Last updated: 21 November 2017
          </div>
        </div><!--//doc-header-->

        <div class="doc-body">
          <div class="doc-content">
            <div class="content-inner">

              <h2 id="tools-overview">Tools overview</h2>

<p>Marian toolkit provides the following tools:</p>

<ul>
  <li><code class="highlighter-rouge">marian</code>: for training models of all types</li>
  <li><code class="highlighter-rouge">marian-decoder</code>: for GPU translation with models of all types</li>
  <li><code class="highlighter-rouge">marian-server</code>: the web-socket server providing GPU translation</li>
  <li><code class="highlighter-rouge">marian-scorer</code>: the rescoring tool</li>
  <li><code class="highlighter-rouge">amun</code>: for CPU and GPU translation with Amun and certain Nematus models</li>
</ul>

<h3 id="model-types">Model types</h3>

<p>Available model types:</p>
<ul>
  <li><code class="highlighter-rouge">s2s</code>: Default model type, which supports most of the features provided by
the toolkit. The architecture is equivalent to the
<a href="https://github.com/EdinburghNLP/nematus">Nematus</a> models (<a href="https://arxiv.org/abs/1703.04357">Senrich et al.,
2017</a>), which use the RNN
encoder-decoder architecture with an attention mechanism, but not fully
compatible with them. Certain models of this type can be converted to models
of type Amun.</li>
  <li><code class="highlighter-rouge">multi_s2s</code>: Model of type <code class="highlighter-rouge">s2s</code>, which uses two or more encoders enabling
multi-source neural translation.</li>
  <li><code class="highlighter-rouge">transformer</code>: A new model based on <a href="https://arxiv.org/abs/1706.03762"><em>Attention is all you need</em>, Vaswani et
al., 2017</a>.</li>
  <li><code class="highlighter-rouge">amun</code>: Model architecture is equivalent to the DL4MT models used in Nematus.
Can be decoded with Amun tool as <em>nematus</em> model type.</li>
  <li><code class="highlighter-rouge">nematus</code>: Model architecture is equivalent to deep models developed by the
Edinburgh MT group for WMT 2017 using Nematus toolkit. This is the only model
type supporting models trained with the Nematus-style layer normalization.
Can be decoded with the Amun tool as <em>nematus2</em> model type.</li>
  <li><code class="highlighter-rouge">lm</code>: An RNN language model.</li>
</ul>

<h2 id="training">Training</h2>

<p>For training NMT models, you want to use <code class="highlighter-rouge">marian</code> command.
Assuming <code class="highlighter-rouge">corpus.en</code> and <code class="highlighter-rouge">corpus.ro</code> are corresponding and preprocessed files
of a English-Romanian parallel corpus, the following command will create a
Nematus-compatible neural machine translation model:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>./build/marian \
    --train-sets corpus.en corpus.ro \
    --vocabs vocab.en vocab.ro \
    --model model.npz
</code></pre>
</div>

<p>Training settings can be provided in the configuration file:</p>

<div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="c1"># config.yml</span>
<span class="s">train-sets</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">corpus.en</span>
  <span class="pi">-</span> <span class="s">corpus.ro</span>
<span class="s">vocabs</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">vocab.en</span>
  <span class="pi">-</span> <span class="s">vocab.ro</span>
<span class="s">model</span><span class="pi">:</span> <span class="s">model.npz</span>
</code></pre>
</div>

<p>which simplifies the command to:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>./build/marian -c config.yml
</code></pre>
</div>

<p>Command-line options overwrite options stored in the configuration file.</p>

<h3 id="multi-gpu-training">Multi-GPU training</h3>

<p>Use option <code class="highlighter-rouge">--devices</code> to specify on which GPU devices the model should be
trained:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>./build/marian \
    --devices 0 1 2 3 \
    --train-sets corpus.en corpus.ro \
    --vocabs vocab.en vocab.ro \
    --model model.npz
</code></pre>
</div>

<p>The asynchronous SGD is used by default, while the synchronous version can be
enabled with <code class="highlighter-rouge">--sync-sgd</code>. The latter scales worse on multiple devices, but may
allow to achieve better cross-entropy scores.</p>

<h3 id="validation">Validation</h3>

<p>It is useful to monitor the performance of your model during training on
held-out data.</p>

<p>This is a minimum example of how to validate the model using cross-entropy and
BLEU score:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>./build/marian \
    --train-sets corpus.en corpus.ro \
    --vocabs vocab.en vocab.ro \
    --model model.npz \
    --valid-set dev.en dev.ro \
    --valid-metrics cross-entropy translation \
    --valid-script-path validate.sh
</code></pre>
</div>

<p>where <em>validate.sh</em> is a bash script, which takes the file with output
translation of <code class="highlighter-rouge">dev.en</code> as the first argument (i.e. <code class="highlighter-rouge">$1</code>) and returns the BLEU
score, e.g.:</p>

<div class="language-sh highlighter-rouge"><pre class="highlight"><code><span class="c"># validate.sh</span>
cat <span class="nv">$1</span> | ./postprocess.sh 2&gt;/dev/null &gt; file.out
./moses-scripts/scripts/generic/multi-bleu-detok.perl file.ref &lt; file.out 2&gt;/dev/null <span class="se">\</span>
    | sed -r <span class="s1">'s/BLEU = ([0-9.]+),.*/\1/'</span>
</code></pre>
</div>

<h3 id="decaying-learning-rate">Decaying learning rate</h3>

<p>Manipulation of learning rate during the training may result in better
convergence and higher-quality translations.</p>

<p>Marian supports various strategies for decaying learning rate
(<code class="highlighter-rouge">--lr-decay-strategy</code> option):</p>
<ul>
  <li><code class="highlighter-rouge">epoch</code>: learning rate will be decayed after each epoch starting from epoch
specified with <code class="highlighter-rouge">--lr-decay-start</code></li>
  <li><code class="highlighter-rouge">batches</code>: learning rate will be decayed every <code class="highlighter-rouge">--lr-decay-freq</code> batches
starting after the batch specified with <code class="highlighter-rouge">--lr-decay-start</code></li>
  <li><code class="highlighter-rouge">stalled</code>: learning rate will be decayed every time when the first validation
metric does not improve for <code class="highlighter-rouge">--lr-decay-start</code> consecutive validation steps</li>
  <li><code class="highlighter-rouge">epoch+stalled</code>: learning rate will be decayed after the specified number of
epochs or stalled validation steps, whichever comes first. The option
<code class="highlighter-rouge">--lr-decay-start</code> takes two numbers: for epochs and stalled validation
steps, respectively</li>
  <li><code class="highlighter-rouge">batches+stalled</code>: as previous strategy, but batches are used instead of
epochs</li>
</ul>

<p>Decay factor for learning rate can be specified with <code class="highlighter-rouge">--lr-decay</code>.</p>

<h2 id="translation">Translation</h2>

<p>All model types can be decoded with <code class="highlighter-rouge">marian-decoder</code> and <code class="highlighter-rouge">marian-server</code>
command.  Only models of type Amun and certain models of type Nematus can be
used with the <code class="highlighter-rouge">amun</code> command line tool.</p>

<h3 id="marian-decoder">Marian decoder</h3>

<p>Currently, <code class="highlighter-rouge">marian-decoder</code> supports only translation on GPUs. A CPU version
of Marian is planned.</p>

<p>Basic usage:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>./build/marian-decoder -m model.npz -v vocab.en vocab.ro &lt; input.txt
</code></pre>
</div>

<h4 id="model-ensembling">Model ensembling</h4>

<p>Models of various types and architectures can be ensembled if they use the same
vocabularies:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>./build/marian-decoder \
    --models model1.npz model2.npz model3.npz \
    --weights 0.6 0.2 0.2 \
    --vocabs vocab.en vocab.ro &lt; input.txt
</code></pre>
</div>

<p>Weights are optional and set to 1.0 by default if ommitted.</p>

<h4 id="recommended-settings-for-translation">Recommended settings for translation</h4>

<p>We found that using length normalization with a penalty term of 0.6 and a beam
size of 6 is usually best. This rougly follows the settings by Google from
their <a href="https://arxiv.org/abs/1706.03762">transformer paper</a>.  Assuming your
model file is <code class="highlighter-rouge">model.npz</code> and your vocabulary is <code class="highlighter-rouge">vocab.src.yml</code> and
<code class="highlighter-rouge">vocab.trg.yml</code>, we recommend to use the following options:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>./marian-decoder -m model.npz -v vocab.src.yml vocab.trg.yml -b 6 --normalize=0.6
</code></pre>
</div>

<h4 id="batched-translation">Batched translation</h4>

<p>This a feature introduced in Marian v1.1.0. Batched translation generates
translation for whole mini-batches and significantly increases translation
speed (roughly by a factor of 10 or more). Assuming your model file is
<code class="highlighter-rouge">model.npz</code> and your vocabulary is <code class="highlighter-rouge">vocab.src.yml</code> and <code class="highlighter-rouge">vocab.trg.yml</code>, we
recommend to use the following options to enable batched translation:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>./marian-decoder -m model.npz -v vocab.src.yml vocab.trg.yml -b 6 --normalize=0.6 --mini-batch 64 --maxi-batch-sort src --maxi-batch 100 -w 2500
</code></pre>
</div>

<p>This does a number of things: firstly, it enables translation with a mini-batch
size of 64, i.e. translating 64 sentences at once, with a beam-size of 6; It
preloads 100 maxi-batches and sorts them according to source sentence length,
this allows for better packing of the batch and increases translation speed
quite a bit. We also added an option to use a length-normalization weight of
0.6 (this usually increases BLEU a bit) and set the working memory to 2500 MB.
The default working memory is 512 MB and Marian will increase it to match to
requirements during translation, but pre-allocating memory makes it usually a
bit faster.</p>

<p>To give you an idea, how much faster batched translation is compared to
sentence-by-sentence translation we have collected a few numbers. Proper
benchmarks will be added soon. Below we have compiled the time it takes to
translate the English-German WMT2013 test set with 3000 sentences using 4 Volta
GPUs on AWS.</p>

<table class="table table-bordered table-striped">
  <thead>
    <tr>
      <th>System</th>
      <th>Single</th>
      <th>Batched</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Nematus-style Shallow RNN</td>
      <td>82.7s</td>
      <td>4.3s</td>
    </tr>
    <tr>
      <td>Nematus-style Deep RNN</td>
      <td>148.5s</td>
      <td>5.9s</td>
    </tr>
    <tr>
      <td>Google Transformer</td>
      <td>201.9s</td>
      <td>19.2s</td>
    </tr>
  </tbody>
</table>

<h4 id="models-trained-with-nematus">Models trained with Nematus</h4>

<p>Certain types of models trained with Nematus, for example the <a href="http://data.statmt.org/wmt17_systems/">Edinburgh WMT17
deep models</a> can be decoded with
<code class="highlighter-rouge">marian-decoder</code>.  As such models do not include model parameters specifying
the model architecture, all parameters have to be set with command-line
options.
For example, for the <a href="http://data.statmt.org/wmt17_systems/en-de/">de-en model</a>
this would be:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>./build/marian-decoder \
    --type nematus \
    --models model/en-de/model.npz \
    --vocabs model/en-de/vocab.en.json model/en-de/vocab.de.json \
    --dim-vocabs 51100 74383 \
    --enc-depth 1 \
    --enc-cell-depth 4 \
    --enc-type bidirectional \
    --dec-depth 1 \
    --dec-cell-base-depth 8 \
    --dec-cell-high-depth 1 \
    --dec-cell gru-nematus --enc-cell gru-nematus \
    --tied-embeddings true \
    --layer-normalization true
</code></pre>
</div>

<p>Alternatively, the model parameters can be added into the model <em>.npz</em> file
based on the Nematus <em>.json</em> file using the script: <a class="github-link" href="https://github.com/marian-nmt/marian-dev/tree/master/scripts/contrib/inject_model_params.py " target="_blank"><code class="highlighter-rogue">marian-dev/scripts/contrib/inject_model_params.py </code></a>, e.g.:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>python inject_model_params.py -m model.npz -j model.npz.json
</code></pre>
</div>

<h3 id="marian-web-server">Marian web server</h3>

<p>The <code class="highlighter-rouge">marian-server</code> command starts a web-socket server for translation.
It uses the same command-line options as <code class="highlighter-rouge">marian-decoder</code>.
The only addition is <code class="highlighter-rouge">--port</code> option, which specifies the port number:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>./build/marian-server --port 8080 -m model.npz -v vocab.en vocab.ro
</code></pre>
</div>

<p>An example client written in Python is <a class="github-link" href="https://github.com/marian-nmt/marian-dev/tree/master/scripts/server/client_example.py " target="_blank"><code class="highlighter-rogue">marian-dev/scripts/server/client_example.py </code></a>:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>./scripts/server/client_example.py -p 8080 &lt; input.txt
</code></pre>
</div>

<h3 id="amun">Amun</h3>

<p>Amun is a translation tool for certain models of <code class="highlighter-rouge">amun</code> and <code class="highlighter-rouge">nematus</code> model
types. Translation can be performed on GPU or CPU or both.</p>

<p>Basic usage:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>./marian/build/amun -m model.npz -s vocab.en -t vocab.ro &lt;&lt;&lt; "This is a test ."
</code></pre>
</div>

<h2 id="scorer">Scorer</h2>

<p>The <code class="highlighter-rouge">marian-scorer</code> tool is used for scoring (or re-scoring) parallel sentences
provided as plain texts in two corresponding files:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>./build/marian-scorer -m model.npz -v vocab.ro vocab.en -t file.ro file.en
</code></pre>
</div>

<p>A cross-entropy score for each sentence pair is returned by default.  The
scorer can be also used to summarize scores (option <code class="highlighter-rouge">--summary</code>), so it can
calculate cross-entropy and perplexity for a whole test set and report it at
the end.</p>

<h3 id="n-best-lists">N-best lists</h3>

<p>The scorer does not support n-best lists as an input yet.</p>

<h2 id="command-line-options">Command-line options</h2>

<ul>
  <li><a href="/docs/cmd/marian">marian</a></li>
  <li><a href="/docs/cmd/marian-decoder">marian-decoder</a></li>
  <li><a href="/docs/cmd/marian-server">marian-server</a></li>
  <li><a href="/docs/cmd/marian-scorer">marian-scorer</a></li>
  <li><a href="/docs/cmd/amun">amun</a></li>
</ul>

<h2 id="code-documentation">Code documentation</h2>

<p><a href="/docs/marian/classes.html">The code documentation for Marian toolkit</a> is
generated using Doxygen. The newest version can be generated locally with CMake:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>mkdir -p build &amp;&amp; cd build &amp;&amp; cmake .. &amp;&amp; make doc
</code></pre>
</div>


            </div><!--//content-inner-->
          </div><!--//doc-content-->

          <div class="doc-sidebar hidden-xs">
            <nav id="doc-nav"></nav>
          </div><!--//doc-sidebar-->

        </div><!--//doc-body-->

      </div><!--//container-->
    </div><!--//doc-wrapper-->

    </div><!--//page-wrapper-->

    <footer id="footer" class="footer text-center">
  <div class="container">
    <p>
     Marian - an efficient Neural Machine Translation framework written in pure C++.</br>
      Mainly developed at the Adam Mickiewicz University in Pozna≈Ñ and at the University of Edinburgh.
    </p>
    <p><a href="https://github.com/marian-nmt/marian">Marian</a> is licensed under the <a href="https://github.com/marian-nmt/marian/LICENSE">MIT license</a>.</p>
    <small class="copyright">Based on the theme PrettyDocs designed by <a href="http://themes.3rdwavemedia.com/" targe="_blank">Xiaoying Riley</a> with modifications.</small>
  </div><!--//container-->
</footer><!--//footer-->

    <!-- Main Javascript -->
<script type="text/javascript"> localStorage.clear();</script>

<script type="text/javascript" src="/assets/plugins/jquery-1.12.3.min.js"></script>
<script type="text/javascript" src="/assets/plugins/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/assets/plugins/jquery-scrollTo/jquery.scrollTo.min.js"></script>
<script type="text/javascript" src="/assets/plugins/jquery-match-height/jquery.matchHeight-min.js"></script>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/javascript" src="/assets/js/main.js"></script>
<script type="text/javascript" src="/assets/js/toc.js"></script>


  </body>
</html>
