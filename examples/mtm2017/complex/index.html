<!--[if IE 8]> <html lang="en" class="ie8"> <![endif]-->
<!--[if IE 9]> <html lang="en" class="ie9"> <![endif]-->
<!--[if !IE]><!-->
<html lang="en">
<!--<![endif]-->

  <head>
  <title>
    
    Marian :: MTM2017 Tutorial - Part 2
    
  </title>
  <!-- Meta -->
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Fast Neural Machine Translation in C++">

  <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
  <!-- Global CSS -->
  <link rel="stylesheet" href="/assets/plugins/bootstrap/css/bootstrap.min.css">
  <!-- Plugins CSS -->
  <link rel="stylesheet" href="/assets/plugins/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="/assets/css/pygments/github.css">

  <!-- Theme CSS -->
  <link id="theme-style" rel="stylesheet" href="/assets/css/styles.css">
  <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->

  <link rel="stylesheet" href="/assets/plugins/github-fork-ribbon-css/gh-fork-ribbon.css" />
  <!--[if lt IE 9]>
    <link rel="stylesheet" href="/assets/plugins/github-fork-ribbon-css/gh-fork-ribbon.ie.css" />
  <![endif]-->

  

  
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109819276-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-109819276-1');
</script>

  

</head>


  <body class="body-blue">
    <a class="github-fork-ribbon" href="https://github.com/marian-nmt/marian" title="Fork me on GitHub">Fork me on GitHub</a>

    <div class="page-wrapper">

    <header id="header" class="header">
  <div class="container">
    <div class="branding">
      <h1 class="logo">
        <a href="/">
          <span aria-hidden="true" class="icon_documents_alt icon"></span>
          <span class="text-highlight">Marian</span><span class="text-bold">NMT</span>
        </a>
      </h1>
      <p class="description">Fast Neural Machine Translation in C++</p>
    </div><!--//branding-->

    <ol class="breadcrumb">


 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 
 <li>
   <a class="page-link" href="/quickstart/">Quick start</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/features/">Features &amp; Benchmarks</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/docs/">Documentation</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/examples/">Examples</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/faq">FAQ</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/publications/">Publications</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/blog">Blog</a>
 </li>
 

</ol>


  </div><!--//container-->
</header><!--//header-->


    <div class="doc-wrapper">
      <div class="container">

        <div id="doc-header" class="doc-header text-center">
          <h1 class="doc-title">
            
            <i class="icon fa fa-cogs }}"></i>
            
            MTM2017 Tutorial - Part 2
          </h1>
          <div class="meta">
            <i class="fa fa-clock-o"></i>
            Last updated: 07 November 2017
          </div>
        </div><!--//doc-header-->

        <div class="doc-body">
          <div class="doc-content">
            <div class="content-inner">

              <h1 id="more-complex-models-with-marian">More complex models with Marian</h1>

<h2 id="deep-models-edinburgh-wmt-2017-style">Deep models (Edinburgh WMT 2017 style)</h2>

<p>In the upcoming WMT2017 paper from Edinburgh
<em><a href="https://arxiv.org/abs/1707.07631">Deep Architectures for Neural Machine Translation</a></em>
the authors compare multiple known and novel deep architecturs. Marian implements
most of them as well as the tricks used to achieve faster convergence and smaller
models.</p>

<h3 id="layer-normalization">Layer normalization</h3>

<p>Layer normalization is a technique that greatly improves convergence of RNNs
(according to our own experiments by a factor 3-5) and also results in better
quality at test time. It is comparable to feature normalization where features
are normalized by subtracting their mean and dividing them by their variance. Instead
of only applying it to features, layer-normalization is applied to the activations
of the neural network. See <a href="https://arxiv.org/abs/1607.06450">here</a> for more details.</p>

<p>Layer normalization is turned on by:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>--layer-normalization
</code></pre></div></div>

<p>or in the config file with:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>layer-normalization: true
</code></pre></div></div>

<h3 id="tied-embeddings">Tied embeddings</h3>

<p>As the discussed models can become quite large, the tying of embedding matrices
can help to reduce models size and memory footprints during training.
As <a href="https://arxiv.org/abs/1608.05859">Press&amp;Wolf</a> show, tying target embeddings and
the last layer of the output does not decrease quality and helps saving significant
amounts of parameters. Activated by:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>--tied-embeddings
</code></pre></div></div>

<p>or</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tied-embeddings: true
</code></pre></div></div>

<h3 id="deep-decoders">Deep decoders</h3>

<p>The authors investigate two types of depth for the decoder.
Multi-layer RNNs and complex RNN cells that can again consist of cell-like
feed-forward layers. In the decoder the first RNN layer (the conditional cell that
contains the attention mechanism) and all other layers have separate depth settings.</p>

<p>We set the number of decoder layers with</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>--dec-depth 4
</code></pre></div></div>

<p>the number of feedword layers with the cell of the first and the following layers with</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>--dec-cell-base-depth 4
--dec-cell-high-depth 2
</code></pre></div></div>

<p>Alternatively, this can again be set in the config file:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dec-depth: 4
dec-cell-base-depth: 4
dec-cell-high-depth: 4
</code></pre></div></div>

<h3 id="deep-encoders">Deep encoders</h3>

<p>As for decoders, encoders can have multiple layers of complex RNN cells to be set
with:</p>

<p>We set the number of decoder layers with</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>--enc-depth 4
--dec-cell-depth 2
</code></pre></div></div>

<p>or</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>enc-depth: 4
enc-cell-depth: 2
</code></pre></div></div>

<p>Futhermore, the authors of the Edinburgh paper take a look at different encoder
types:</p>

<ul>
  <li><strong>Bidirectional</strong>: consists of a left-to-right and a right-to-left RNN that can
have mulitple layers of complex cells. The final output is concatenated. Activated by
<code class="highlighter-rouge">--enc-type bidirectional</code> or <code class="highlighter-rouge">enc-type: bidirectional</code>.</li>
  <li><strong>Bi-unidirectional</strong>: Consists of a left-to-right and a right-to-left RNN with a
single layer of potentially complex cells. The outputs of the first layer are concatened
and then fed into more uni-directinal layers. Activated by
<code class="highlighter-rouge">--enc-type bi-unidirectional</code> or <code class="highlighter-rouge">enc-type: bi-unidirectional</code>.</li>
  <li><strong>Alternating</strong>: Consists of two RNNs with alternating directionality per layer,
the final outputs are concatenated, used with <code class="highlighter-rouge">--enc-type alternating</code> or <code class="highlighter-rouge">enc-type: alternating</code>.</li>
</ul>

<p>For <code class="highlighter-rouge">--enc-depth 1</code>, all three encoder types are reduced to the same single layer
bidirectional RNN.</p>

<h3 id="residual-connections">Residual connections</h3>

<p>Residual connection allow to skip over layers by calculating <code class="highlighter-rouge">y = f(x) + x</code> where
<code class="highlighter-rouge">f</code> is the function represented by a layer. It is generally believed that residual
connections improve learning in deeper networks.</p>

<p>We activate residual connection between RNN layers on the command line with</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>--skip
</code></pre></div></div>

<p>or in the config file</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>skip: true
</code></pre></div></div>

<h3 id="alternative-rnn-cells">Alternative RNN cells</h3>

<p>During our own experiments, we found that LSTM cells work better for deeper
models (GRU cell seem to work better for the default shallow model).
The LSTM cell can be chosen separately for the encoder and decoder with the
following switches</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>--enc-cell lstm --dec-cell lstm
</code></pre></div></div>

<p>or with the yaml config file entries</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>enc-cell: lstm
dec-cell: lstm
</code></pre></div></div>

<h3 id="putting-it-all-together">Putting it all together</h3>

<p>This model will of course use a lot more memory on the GPU as the shallow models
 trained earlier. We increase the workspace memory to 6000 MB and create the config
 file as follows:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mkdir -p model.deep

./marian-dev/build/marian \
  --type s2s \
  --train-set data/corpus.clean.bpe.ro data/corpus.clean.bpe.en \
  --valid-set data/newsdev2016.bpe.ro data/newsdev2016.bpe.en \
  --vocabs data/vocab.ro.yml data/vocab.en.yml \
  --model model.deep/model.npz \
  --enc-depth 4 --enc-type alternating --enc-cell lstm --enc-cell-depth 2 \
  --dec-depth 4 --dec-cell lstm --dec-cell-base-depth 4 --dec-cell-high-depth 2 \
  --tied-embeddings --layer-normalization --skip \
  --dim-vocabs 66000 50000 \
  --mini-batch-fit --workspace 6500 \
  --dropout-rnn 0.2 --dropout-src 0.1 --exponential-smoothing \
  --early-stopping 5 --disp-freq 1000 \
  --log model.deep/train.log --valid-log model.deep/valid.log \
  --dump-config &gt; model.deep/config.yml
</code></pre></div></div>

<p>As before, we can just use the config file to start our training process:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./marian-dev/build/marian -c model.deep/config.yml
</code></pre></div></div>

<p>Translation is done in the same way as for the shallow models. The model file
constains information on its own architecture and the parameters can be ommitted
on the command line.</p>

<h2 id="optional-dual-encoder-models-for-automatic-post-editing">Optional: dual-encoder models for automatic post-editing</h2>

<p>Various models with multiple encoders and different attention mechanisms are
discussed in our paper <em>An Exploration of Neural Sequence-to-Sequence Architectures for Automatic Post-Editing</em>
available <a href="https://arxiv.org/abs/1706.04138">here</a>.</p>

<p>A first version (not quite done yet) of a companion page with config files for Marian and data to train multi-encoder
models is available <a href="/examples/exploration/">here</a>.</p>

<p>Back to <strong><a href="/examples/mtm2017/intro/">Part 1: First steps with Marian</a></strong></p>

<p>Continue with <strong><a href="/examples/mtm2017/code/">Part 3: A coding tutorial</a></strong></p>


            </div><!--//content-inner-->
          </div><!--//doc-content-->

          <div class="doc-sidebar hidden-xs">
            <nav id="doc-nav"></nav>
          </div><!--//doc-sidebar-->

        </div><!--//doc-body-->

      </div><!--//container-->
    </div><!--//doc-wrapper-->

    </div><!--//page-wrapper-->

    <footer id="footer" class="footer text-center">
  <div class="container">
    <p>
     Marian - an efficient Neural Machine Translation framework written in pure C++.</br>
      Mainly developed at Microsoft Translator and at the University of Edinburgh.
    </p>
    <p><a href="https://github.com/marian-nmt/marian#marian">Marian</a> is licensed under the <a href="https://github.com/marian-nmt/marian/blob/master/LICENSE.md">MIT license</a>.</p>
    <p><small class="copyright footnote">Based on the theme PrettyDocs designed by <a href="http://themes.3rdwavemedia.com/" targe="_blank">Xiaoying Riley</a> with modifications.</small></p>
  </div><!--//container-->
</footer><!--//footer-->

    <!-- Main Javascript -->
<script type="text/javascript"> localStorage.clear();</script>

<script type="text/javascript" src="/assets/plugins/jquery-1.12.3.min.js"></script>
<script type="text/javascript" src="/assets/plugins/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/assets/plugins/jquery-scrollTo/jquery.scrollTo.min.js"></script>
<script type="text/javascript" src="/assets/plugins/jquery-match-height/jquery.matchHeight-min.js"></script>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/javascript" src="/assets/js/main.js"></script>
<script type="text/javascript" src="/assets/js/toc.js"></script>


  </body>
</html>
