<!--[if IE 8]> <html lang="en" class="ie8"> <![endif]-->
<!--[if IE 9]> <html lang="en" class="ie9"> <![endif]-->
<!--[if !IE]><!-->
<html lang="en">
<!--<![endif]-->

  <head>
  <title>
    
    Marian :: MTM2017 Tutorial - Part 1
    
  </title>
  <!-- Meta -->
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Fast Neural Machine Translation in C++">

  <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
  <!-- Global CSS -->
  <link rel="stylesheet" href="/assets/plugins/bootstrap/css/bootstrap.min.css">
  <!-- Plugins CSS -->
  <link rel="stylesheet" href="/assets/plugins/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="/assets/css/pygments/github.css">

  <!-- Theme CSS -->
  <link id="theme-style" rel="stylesheet" href="/assets/css/styles.css">
  <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->

  <link rel="stylesheet" href="/assets/plugins/github-fork-ribbon-css/gh-fork-ribbon.css" />
  <!--[if lt IE 9]>
    <link rel="stylesheet" href="/assets/plugins/github-fork-ribbon-css/gh-fork-ribbon.ie.css" />
  <![endif]-->

  

  
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-109819276-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-109819276-1');
</script>

  

</head>


  <body class="body-blue">
    <a class="github-fork-ribbon" href="https://github.com/marian-nmt/marian" title="Fork me on GitHub">Fork me on GitHub</a>

    <div class="page-wrapper">

    <header id="header" class="header">
  <div class="container">
    <div class="branding">
      <h1 class="logo">
        <a href="/">
          <span aria-hidden="true" class="icon_documents_alt icon"></span>
          <span class="text-highlight">Marian</span><span class="text-bold">NMT</span>
        </a>
      </h1>
      <p class="description">Fast Neural Machine Translation in C++</p>
    </div><!--//branding-->

    <ol class="breadcrumb">


 

 

 

 

 

 

 

 

 

 

 

 

 

 

 
 <li>
   <a class="page-link" href="/quickstart/">Quick start</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/features/">Features &amp; Benchmarks</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/docs/">Documentation</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/examples/">Examples</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/faq">FAQ</a>
 </li>
 

 
 <li>
   <a class="page-link" href="/publications/">Publications</a>
 </li>
 

</ol>


  </div><!--//container-->
</header><!--//header-->


    <div class="doc-wrapper">
      <div class="container">

        <div id="doc-header" class="doc-header text-center">
          <h1 class="doc-title">
            
            <i class="icon fa fa-cogs }}"></i>
            
            MTM2017 Tutorial - Part 1
          </h1>
          <div class="meta">
            <i class="fa fa-clock-o"></i>
            Last updated: 07 November 2017
          </div>
        </div><!--//doc-header-->

        <div class="doc-body">
          <div class="doc-content">
            <div class="content-inner">

              <p><b>Marian</b> is an efficient Neural Machine Translation framework written
  in pure C++ with minimal dependencies. It has mainly been developed at the
  Adam Mickiewicz University in Poznań (AMU) and at the University of Edinburgh.</p>

<p>It is currently being deployed in multiple European projects and is the main
  translation and training engine behind the neural MT launch at the
  <a href="http://www.wipo.int/pressroom/en/articles/2016/article_0014.html">World Intellectual Property Organization</a>.</p>

<p>
  Main features:
  <ul>
    <li> Fast multi-gpu training and translation </li>
    <li> Compatible with Nematus and DL4MT </li>
    <li> Efficient pure C++ implementation </li>
    <li> Permissive open source license (MIT) </li>
    <li> <a href="../../features"> more details... </a> </li>
  </ul>
  </p>

<p>It is also a Machine Translation Marathon 2016 project that is celebrating its
  first birthday during the current MTM :)</p>

<h1 id="first-steps-with-marian">First steps with Marian</h1>

<p>Change your current working directory to mtm2017-marian</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cd mtm2017-marian
</code></pre></div></div>

<h2 id="checkout-and-compilation">Checkout and Compilation</h2>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/marian-nmt/marian-dev
cd marian-dev
mkdir build
cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
make -j
</code></pre></div></div>

<p>If everything worked correctly you can display the list of options with</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./marian --help
</code></pre></div></div>

<p>And download a couple of useful scripts:</p>

<p>Return to the working directory and download a number of scripts for
preprocessing and splitting into subwords:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cd ../..

git clone https://github.com/marian-nmt/mtm2017-tutorial
git clone https://github.com/marian-nmt/moses-scripts
git clone https://github.com/rsennrich/subword-nmt
</code></pre></div></div>

<h2 id="tanslate-with-a-pretrained-model">Tanslate with a pretrained model</h2>

<p>We will first preprocess test files for translation. Make sure you understand
what each command is doing (normalise-romanian.py and remove-diacritics.py are
specialized commands for Romanian created by Barry Haddow).</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cat mtm2017-tutorial/test/newstest2016.ro \
    | ./moses-scripts/scripts/tokenizer/normalize-punctuation.perl -l ro \
    | ./mtm2017-tutorial/scripts/normalise-romanian.py \
    | ./mtm2017-tutorial/scripts/remove-diacritics.py \
    | ./moses-scripts/scripts/tokenizer/tokenizer.perl -a -l ro \
    | ./moses-scripts/scripts/recaser/truecase.perl -model pretrained/truecase-model.ro \
    | ./subword-nmt/apply_bpe.py -c pretrained/roen.bpe &gt; pretrained/newstest2016.tok.tc.bpe.ro

cat mtm2017-tutorial/test/newstest2016.en \
    | ./moses-scripts/scripts/tokenizer/normalize-punctuation.perl -l en \
    | ./moses-scripts/scripts/tokenizer/tokenizer.perl -a -l en &gt; pretrained/newstest2016.tok.en
</code></pre></div></div>

<p>We can now translate the given test set with the command below. The
files <code class="highlighter-rouge">vocab.{ro,en}.yml</code> contain the input and output vocabulary, <code class="highlighter-rouge">model.npz</code>
is the model parameter file in Numpy format. Run the command and check
if you can infer the model parameters (number of units in the rnn, number of
layers, etc.).</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cat pretrained/newstest2016.tok.tc.bpe.ro | \
  marian-dev/build/s2s -m pretrained/model.npz -v pretrained/vocab.ro.yml pretrained/vocab.en.yml \
  &gt; pretrained/newstest2016.tok.tc.bpe.ro.output
</code></pre></div></div>

<p>The output <code class="highlighter-rouge">pretrained/newstest2016.tok.tc.bpe.ro.output</code> needs to be
post-processed in order to compare it to the tokenized reference.
We fuse subwords back together, detokenize and uppercase the first letter in each line:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cat pretrained/newstest2016.tok.tc.bpe.ro.output | \
  perl -pe 's/@@ //g' | \
  perl ./moses-scripts/scripts/recaser/detruecase.perl \
  &gt; pretrained/newstest2016.tok.ro.output
</code></pre></div></div>

<p>After that we can compute the BLEU score for this translation:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>perl ./moses-scripts/scripts/generic/multi-bleu.perl pretrained/newstest2016.tok.en &lt; pretrained/newstest2016.tok.ro.output
</code></pre></div></div>

<h2 id="training-your-own-wmt-grade-model">Training your own WMT-grade model</h2>

<p>If you want to learn how to prepare your training data for a Romanian-English
NMT system, you should continue with the next section <em>Preprocessing your
data</em> and step through all the commands to create the training,
development and test data. This may however take a while, therefore I recommend
to skip it first and come back later.</p>

<p>If you skip this step, just rename the <code class="highlighter-rouge">data.done</code> folder to <code class="highlighter-rouge">data</code> and
move on to the following section <em>Training a basic Nematus-style model</em>.</p>

<h3 id="preprocessing-your-data">Preprocessing your data</h3>

<p>Download the training data (this includes back-translated data by Rico Sennrich):</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mkdir -p data
wget http://www.statmt.org/europarl/v7/ro-en.tgz -O data/ro-en.tgz
wget http://opus.lingfil.uu.se/download.php?f=SETIMES2/en-ro.txt.zip -O data/SETIMES2.ro-en.txt.zip
wget http://data.statmt.org/rsennrich/wmt16_backtranslations/ro-en/corpus.bt.ro-en.en.gz -O data/corpus.bt.ro-en.en.gz
wget http://data.statmt.org/rsennrich/wmt16_backtranslations/ro-en/corpus.bt.ro-en.ro.gz -O data/corpus.bt.ro-en.ro.gz
</code></pre></div></div>

<p>Unpack and concatenate the training data:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cd data/
tar -xf ro-en.tgz
unzip SETIMES2.ro-en.txt.zip
gzip -d corpus.bt.ro-en.en.gz corpus.bt.ro-en.ro.gz

cat europarl-v7.ro-en.en SETIMES2.en-ro.en corpus.bt.ro-en.en &gt; corpus.en
cat europarl-v7.ro-en.ro SETIMES2.en-ro.ro corpus.bt.ro-en.ro &gt; corpus.ro

cd ..
</code></pre></div></div>

<p>Tokenization and other language specific pre-processing steps:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cat data/corpus.ro \
    | ./moses-scripts/scripts/tokenizer/normalize-punctuation.perl -l ro \
    | ./mtm2017-tutorial/scripts/normalise-romanian.py \
    | ./mtm2017-tutorial/scripts/remove-diacritics.py \
    | ./moses-scripts/scripts/tokenizer/tokenizer.perl -threads 4 -a -l ro &gt; data/corpus.tok.ro

cat data/corpus.en \
    | ./moses-scripts/scripts/tokenizer/normalize-punctuation.perl -l en \
    | ./moses-scripts/scripts/tokenizer/tokenizer.perl -threads 4 -a -l en &gt; data/corpus.tok.en


cat mtm2017-tutorial/test/newsdev2016.ro \
    | ./moses-scripts/scripts/tokenizer/normalize-punctuation.perl -l ro \
    | ./mtm2017-tutorial/scripts/normalise-romanian.py \
    | ./mtm2017-tutorial/scripts/remove-diacritics.py \
    | ./moses-scripts/scripts/tokenizer/tokenizer.perl -threads 4 -a -l ro &gt; data/newsdev2016.tok.ro

cat mtm2017-tutorial/test/newsdev2016.en \
    | ./moses-scripts/scripts/tokenizer/normalize-punctuation.perl -l en \
    | ./moses-scripts/scripts/tokenizer/tokenizer.perl -threads 4 -a -l en &gt; data/newsdev2016.tok.en

cat mtm2017-tutorial/test/newstest2016.ro \
    | ./moses-scripts/scripts/tokenizer/normalize-punctuation.perl -l ro \
    | ./mtm2017-tutorial/scripts/normalise-romanian.py \
    | ./mtm2017-tutorial/scripts/remove-diacritics.py \
    | ./moses-scripts/scripts/tokenizer/tokenizer.perl -threads 4 -a -l ro &gt; data/newstest2016.tok.ro

cat mtm2017-tutorial/test/newstest2016.en \
    | ./moses-scripts/scripts/tokenizer/normalize-punctuation.perl -l en \
    | ./moses-scripts/scripts/tokenizer/tokenizer.perl -threads 4 -a -l en &gt; data/newstest2016.tok.en
</code></pre></div></div>

<p>Clean empty and long sentences, and sentences with high source-target ratio
(training corpus only):</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./moses-scripts/scripts/training/clean-corpus-n.perl data/corpus.tok ro en data/corpus.clean.tok 1 80
</code></pre></div></div>

<p>Train truecaser:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./moses-scripts/scripts/recaser/train-truecaser.perl -corpus data/corpus.clean.tok.ro -model data/truecase-model.ro
./moses-scripts/scripts/recaser/train-truecaser.perl -corpus data/corpus.clean.tok.en -model data/truecase-model.en
</code></pre></div></div>

<p>Apply truecaser to cleaned training corpus:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for prefix in corpus.clean newsdev2016 newstest2016
do
    ./moses-scripts/scripts/recaser/truecase.perl -model data/truecase-model.ro &lt; data/$prefix.tok.ro &gt; data/$prefix.tc.ro
    ./moses-scripts/scripts/recaser/truecase.perl -model data/truecase-model.en &lt; data/$prefix.tok.en &gt; data/$prefix.tc.en
done
</code></pre></div></div>

<p>Train BPE (<a href="https://arxiv.org/abs/1508.07909">Read here about BPEs</a>)</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cat data/corpus.clean.tc.ro data/corpus.clean.tc.en | ./subword-nmt/learn_bpe.py -s 85000 &gt; data/roen.bpe
</code></pre></div></div>

<p>Apply BPE</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for prefix in corpus.clean newsdev2016 newstest2016
do
    ./subword-nmt/apply_bpe.py -c data/roen.bpe &lt; data/$prefix.tc.ro &gt; data/$prefix.bpe.ro
    ./subword-nmt/apply_bpe.py -c data/roen.bpe &lt; data/$prefix.tc.en &gt; data/$prefix.bpe.en
done
</code></pre></div></div>

<h3 id="training-a-basic-nematus-style-model">Training a basic Nematus-style model</h3>

<p>We can now train a model using our previously created training data. We use
<code class="highlighter-rouge">model</code> as our output folder and set the display freqency to 100
(i.e. a status update will be displayed every 100 mini-batch updates). Try to
inspect the <code class="highlighter-rouge">--help</code> option to determine what kind of model will be trained
by default, e.g. what’s the default batch size? or what kind of encoder
is used? is there regularization?</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mkdir -p model

./marian-dev/build/marian \
  --train-set data/corpus.clean.bpe.ro data/corpus.clean.bpe.en \
  --model model/model.npz \
  --disp-freq 100
</code></pre></div></div>

<p>You can kill the training process with the key shortcut <code class="highlighter-rouge">Ctrl+C</code>.</p>

<p>Let’s try a couple of more advanced options (can you infer from the
help menu what these do?). But first, we will create a config file.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./marian-dev/build/marian \
  --type s2s \
  --train-set data/corpus.clean.bpe.ro data/corpus.clean.bpe.en \
  --valid-set data/newsdev2016.bpe.ro data/newsdev2016.bpe.en \
  --vocabs data/vocab.ro.yml data/vocab.en.yml \
  --model model/model.npz \
  --layer-normalization \
  --dim-vocabs 66000 50000 \
  --mini-batch-fit --workspace 3000 \
  --dropout-rnn 0.2 --dropout-src 0.1 --exponential-smoothing \
  --early-stopping 5 --disp-freq 1000 \
  --log model/train.log --valid-log model/valid.log \
  --dump-config &gt; model/config.yml
</code></pre></div></div>

<p>Now we can just use the config file to start our training process:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./marian-dev/build/marian -c model/config.yml
</code></pre></div></div>

<p>The training process will finish after a long time (these AWS GPUs are quite slow)
and will result in a model with similar performance to the pretrained one.</p>

<p>Continue with <strong><a href="/examples/mtm2017/complex/">Part 2: Complex models</a></strong></p>

<p>Continue with <strong><a href="/examples/mtm2017/code/">Part 3: A coding tutorial</a></strong></p>


            </div><!--//content-inner-->
          </div><!--//doc-content-->

          <div class="doc-sidebar hidden-xs">
            <nav id="doc-nav"></nav>
          </div><!--//doc-sidebar-->

        </div><!--//doc-body-->

      </div><!--//container-->
    </div><!--//doc-wrapper-->

    </div><!--//page-wrapper-->

    <footer id="footer" class="footer text-center">
  <div class="container">
    <p>
     Marian - an efficient Neural Machine Translation framework written in pure C++.</br>
      Mainly developed at the Adam Mickiewicz University in Poznań and at the University of Edinburgh.
    </p>
    <p><a href="https://github.com/marian-nmt/marian">Marian</a> is licensed under the <a href="https://github.com/marian-nmt/marian/LICENSE">MIT license</a>.</p>
    <p><small class="copyright">Based on the theme PrettyDocs designed by <a href="http://themes.3rdwavemedia.com/" targe="_blank">Xiaoying Riley</a> with modifications.</small></p>
  </div><!--//container-->
</footer><!--//footer-->

    <!-- Main Javascript -->
<script type="text/javascript"> localStorage.clear();</script>

<script type="text/javascript" src="/assets/plugins/jquery-1.12.3.min.js"></script>
<script type="text/javascript" src="/assets/plugins/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="/assets/plugins/jquery-scrollTo/jquery.scrollTo.min.js"></script>
<script type="text/javascript" src="/assets/plugins/jquery-match-height/jquery.matchHeight-min.js"></script>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/javascript" src="/assets/js/main.js"></script>
<script type="text/javascript" src="/assets/js/toc.js"></script>


  </body>
</html>
